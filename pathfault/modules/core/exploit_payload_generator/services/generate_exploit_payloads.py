import copy
import json
import os
import random
import urllib.parse
from concurrent.futures import ProcessPoolExecutor
import concurrent
import time
import argparse
import importlib.util
from datetime import datetime

import click
from pathfault.logger import setup_logger
from pathfault.modules.core.exploit_payload_generator.logger import setup_smt_logger

from z3 import StringVal
import gc

from pathfault.inconsistency import ContainsType
from pathfault.inconsistency.server import get_server_actions, calculate_combinations, get_all_server_transformation_combinations
from pathfault.inconsistency.tools import BASE_NORMALIZATION, get_expanded_normalization_with_pre_server, ENCODING_MAP
from pathfault.inconsistency.transformation import ReplaceTransformation

from .solver_process.exploit_generation_process import run_subprocess


logger = setup_logger(__name__)
smt_logger = setup_smt_logger(__name__)


def is_percent_encoded(string):
    """Check if a string already contains percent-encoded sequences."""
    return any(val in string for val in ENCODING_MAP.values())


def percent_encode(string, times):
    """
    Perform `times` rounds of percent-encoding on the provided string,
    returning a list of all possible encoding results at each step without duplicates.

    - If the string is not yet percent-encoded: apply ENCODING_MAP-based encoding.
    - If the string is already percent-encoded:
        1. Apply percent_to_percent_encoding
        2. Apply ENCODING_MAP-based encoding
    """
    # Use a set to avoid duplicates
    results = {string}

    for _ in range(times):
        new_results = set()
        for s in results:
            if not is_percent_encoded(s):
                # Apply ENCODING_MAP encoding if not already encoded
                new_results.add(''.join(ENCODING_MAP.get(char, char) for char in s))
            else:
                # If already encoded, apply both percent-to-percent encoding and full ENCODING_MAP encoding
                new_results.add(percent_to_percent_encoding(s))
                new_results.add(''.join(ENCODING_MAP.get(char, char) for char in s))
        results.update(new_results)

    return list(results)


def percent_to_percent_encoding(string, times=1):
    """
    Convert '%' characters to '%25' in the given string.
    - `times` parameter controls how many times to repeat the replacement.
    """
    for _ in range(times):
        string = string.replace('%', '%25')
    return string


def process_transformations(server_actions) -> dict:
    """
    Iterate over each Server-Action and perform transformation expansion.
    """
    transformation_results = {}
    processed_strings = set()

    for idx, server_action in enumerate(server_actions):
        logger.info(f"Starting contains condition expansion for server '{server_action.name}'")

        for transformation in server_action.transformation_list:
            # extract every ContainsType condition from this transformation
            contains_conditions = [
                condition
                for condition in transformation.conditions
                if isinstance(condition, ContainsType)
            ]

            for condition in contains_conditions:
                target_str = condition.condition_str

                if target_str not in processed_strings:
                    logger.info(f"  ContainsType found – target string: '{target_str}'")
                    processed_strings.add(target_str)
                    current_str = target_str  # keep the current string for expansion

                    # count how many previous servers have is_decode == True
                    decode_in_range_count = sum(
                        pre_sa.server.is_decode
                        for pre_sa in server_actions[:idx]
                    )

                    if server_action.server.is_decode:
                        # first encode once (percent_encode returns a list)
                        candidates = percent_encode(current_str, 1)
                        message = "[Percent-encode once]"
                        extended_candidates = set(candidates)

                        # optionally apply additional encodings
                        if decode_in_range_count > 0:
                            for c in candidates:
                                extended_candidates.update(
                                    percent_encode(c, decode_in_range_count)
                                )
                    else:
                        message = "[Keep as-is, then add percent-encoding]"
                        extended_candidates = set()
                        if decode_in_range_count > 0:
                            # encode current_str decode_in_range_count times
                            extended_candidates.update(
                                percent_encode(current_str, decode_in_range_count)
                            )

                    # always include the original string itself
                    extended_candidates.add(current_str)

                    extended_candidates = list(extended_candidates)
                    logger.info(f"    └─ {message} {extended_candidates}")
                    transformation_results[current_str] = extended_candidates

    return transformation_results

def _timed_run(server_actions, target_output, timeout_ms, random_seed):
    """
    Wrapper around run_subprocess that measures only the solver execution time.
    Returns a tuple (result_dict, elapsed_seconds).
    """
    t0 = time.time()
    result = run_subprocess(server_actions, target_output, timeout_ms, random_seed)
    elapsed = time.time() - t0
    return result, elapsed



@click.command("generate-exploit-payloads")
@click.option("--target-path", required=True, help="Target output URL")
@click.option(
    "--surrogate-model",
    "surrogate_model_path",
    required=True,
    help="Path to surrogate model Python file",
)
@click.option(
    "--smt-timeout",
    default=10,
    type=int,
    help="SMT solving timeout in seconds",
)
@click.option(
    "--max-transformation-num",
    default=1,
    type=int,
    help="Maximum number of transformations per server",
)
@click.option(
    "--max-workers",
    default=6,
    type=int,
    help="Number of parallel worker processes",
)
@click.option(
    "--random-seed",
    default=None,
    type=int,
    help="Random seed for deterministic behavior",
)
@click.option(
    "--output",
    default="./pathfault/results/exploit_payload_generator/exploit_generation_output.json",
    help="Output log file path (JSON format)",
)
def cli_generate_exploit_payloads(
    target_path,
    surrogate_model_path,
    smt_timeout,
    max_transformation_num,
    max_workers,
    random_seed,
    output,
):
    """
    Automated generation of path confusion exploits tailored to attacker objectives
    via HTTP message parser discrepancies.
    """
    # Multiprocessing support on Windows/macOS
    import multiprocessing

    multiprocessing.freeze_support()

    # ---- Random Seed Selection ----
    if random_seed is None:
        seed = random.randint(1, 1_000_000)
        logger.info(f"[Random seed selected]: {seed}")
    else:
        seed = random_seed
        logger.info(f"[Random seed provided]: {seed}")
    random.seed(seed)

    # ---- Load Surrogate Model ----
    logger.info(f"[Loading surrogate model from] {surrogate_model_path}")
    spec = importlib.util.spec_from_file_location("surrogate_model", surrogate_model_path)
    surrogate_mod = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(surrogate_mod)
    surrogate_model_list = surrogate_mod.get_surrogate_model()
    logger.info(f"[Surrogate model loaded] {len(surrogate_model_list)} servers found")

    # ---- Initialize Logging Structure ----
    log_data = {
        "options": {
            "output_url": target_path,
            "surrogate_model": surrogate_model_path,
            "smt_timeout": smt_timeout,
            "max_transformation_num": max_transformation_num,
            "max_workers": max_workers,
            "random_seed": seed,
        },
        "steps": {},
        "smt_solving_info": [],
        "filtered_candidates": [],
        "final_transformed_urls": [],
    }

    # ---- Step 1: SMT-solving candidate discovery ----
    logger.info(f"Step 1 started at {datetime.now().isoformat()}")
    step1_start = time.time()
    log_data["steps"]["step1"] = {"start": datetime.now().isoformat()}

    successful_combinations = []

    all_combinations = get_all_server_transformation_combinations(
        surrogate_model_list, max_transformation_num
    )
    random.shuffle(all_combinations)
    logger.info(f"Total combinations (generated): {len(all_combinations)}")

    total_combination_num = calculate_combinations(
        surrogate_model_list, max_transformation_num=max_transformation_num
    )
    logger.info(f"Total combinations (calculated): {total_combination_num}")

    # 1) Submit all SMT tasks to the pool
    future_to_info = {}
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        for idx, combination in enumerate(all_combinations, start=1):
            selected_transforms, normalize_config = combination
            server_actions = copy.deepcopy(
                get_server_actions(surrogate_model_list, selected_transforms, normalize_config)
            )
            summary_info = " | ".join(
                f"{sa.name}(N:{'Y' if sa.normalize else 'N'}, "
                f"T:{', '.join(t.name for t in sa.transformation_list) or 'None'})"
                for sa in server_actions
            )

            future = executor.submit(
                _timed_run,
                server_actions,
                target_path,
                smt_timeout * 1000,
                seed,
            )
            future_to_info[future] = (summary_info, server_actions)

        total_tasks = len(all_combinations)
        logger.info(f"Queued {total_tasks} SMT-solving tasks.")

        # 2) as_completed 로 처리하면서 정확한 elapsed 사용
        completed_count = 0
        for completed in concurrent.futures.as_completed(future_to_info):
            completed_count += 1
            summary_info, server_actions = future_to_info.pop(completed)

            try:
                result, elapsed = completed.result(timeout=smt_timeout)
            except TimeoutError:
                smt_logger.smt_failed(
                    f"[{completed_count}/{total_tasks}] TIMEOUT ‖ {summary_info}"
                )
                log_data["smt_solving_info"].append({
                    "summary_info": summary_info,
                    "elapsed": None,
                    "timestamp": datetime.now().isoformat(),
                    "status": "timeout"
                })
                continue
            except Exception as exc:
                smt_logger.smt_failed(
                    f"[{completed_count}/{total_tasks}] ERROR ({exc}) ‖ {summary_info}"
                )
                log_data["smt_solving_info"].append({
                    "summary_info": summary_info,
                    "elapsed": None,
                    "timestamp": datetime.now().isoformat(),
                    "status": "error"
                })
                continue

            record = {
                "summary_info": summary_info,
                "elapsed": elapsed,
                "timestamp": datetime.now().isoformat()
            }

            if result is None:
                smt_logger.smt_failed(
                    f"[{completed_count}/{total_tasks}] FAILED ({elapsed:.2f}s) ‖ {summary_info}"
                )
                record["status"] = "failed"
            elif not isinstance(result, dict):
                smt_logger.smt_failed(
                    f"[{completed_count}/{total_tasks}] BAD_RESPONSE ({elapsed:.2f}s) ‖ {summary_info}"
                )
                record["status"] = "error"
            elif result.get("result") == "success":
                found_url = result["input_url"]
                successful_combinations.append({
                    "server_actions": server_actions,
                    "input_url": found_url
                })
                smt_logger.smt_success(
                    f"[{completed_count}/{total_tasks}] SUCCESS ({elapsed:.2f}s) ‖ URL={found_url} ‖ {summary_info}"
                )
                record["status"] = "success"
                record["found_url"] = found_url
            else:
                smt_logger.smt_failed(
                    f"[{completed_count}/{total_tasks}] FAILURE ({elapsed:.2f}s) ‖ {summary_info}"
                )
                record["status"] = "failed"

            log_data["smt_solving_info"].append(record)
    # Force garbage collection
    import gc
    gc.collect()

    # Step 1 completion
    step1_end = time.time()
    log_data["steps"]["step1"]["end"] = datetime.now().isoformat()
    log_data["steps"]["step1"]["duration"] = step1_end - step1_start
    log_data["steps"]["step1"]["smt_solving_count"] = completed_count
    logger.info(
        f"Step 1 completed in {step1_end - step1_start:.2f}s; "
        f"{completed_count}/{total_tasks} tasks processed."
    )

    # ── Step 2 : Normalization Expansion ─────────────────────────────────────────
    logger.info("=== Step 2: starting normalization expansion phase ===")
    step2_start = time.time()
    log_data["steps"]["step2"] = {
        "start": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }

    filtered_candidates_log = []
    filtered_combinations = []

    for success in successful_combinations:
        summary_info = " | ".join(
            f"{sa.name}(normalize={'Y' if sa.normalize else 'N'}, "
            f"transformations={','.join(t.name for t in sa.transformation_list) or 'None'})"
            for sa in success["server_actions"]
        )

        valid_combination = True
        for server_action in success["server_actions"]:
            essentials = server_action.server.essential_transformation_list
            if not all(e in server_action.transformation_list for e in essentials):
                logger.warning(
                    "Filtering out combination: server '%s' is missing an essential transformation",
                    server_action.name
                )
                valid_combination = False
                break

        filtered_candidates_log.append({
            "summary": summary_info,
            "satisfied": valid_combination,
            "input_url": success["input_url"],
        })
        if valid_combination:
            filtered_combinations.append(success)

    log_data["filtered_candidates"] = filtered_candidates_log

    all_normalization_results = []  # store outputs produced in Step 2

    for success in filtered_combinations:
        # list that will finally hold all expanded-normalization candidates
        final_expanded = []
        original_input_url = str(success["input_url"])

        logger.info("[🔹 Step 2 – start normalization expansion]")
        logger.info(f"  🌐 Original URL : {original_input_url}")

        if any(sa.normalize for sa in success["server_actions"]):

            # iterate over every Server-Action that has normalization enabled
            for idx, server_action in enumerate(success["server_actions"]):
                if server_action.normalize:
                    logger.info(f"{server_action.name} – begin normalization expansion")

                    # 1) Decode expansion
                    logger.debug("  running decode expansion …")
                    decode_expanded = server_action.server.get_expanded_normalization_with_decode(
                        BASE_NORMALIZATION
                    )
                    logger.info(f"    decode expansion produced {len(decode_expanded)} candidates")
                    for d in decode_expanded:
                        logger.debug(f"       • {d.transformation_type.normalization_str}")

                    # 2) Replace expansion
                    logger.debug("  running replace expansion …")
                    replace_expanded = []
                    for trans in server_action.server.transformation_list:
                        if isinstance(trans.transformation_type, ReplaceTransformation):
                            newly_expanded = server_action.server.get_expanded_normalization_with_replace(
                                trans.transformation_type
                            )
                            logger.info(
                                f"    transformation '{trans.name}' produced {len(newly_expanded)} candidates"
                            )
                            for r in newly_expanded:
                                logger.debug(f"       • {r.transformation_type.normalization_str}")
                            replace_expanded.extend(newly_expanded)

                    # merge decode + replace expansion results
                    current_expanded = list(set(decode_expanded) | set(replace_expanded))
                    logger.info(f"  merged total {len(current_expanded)} candidates")

                    # initialise / update final_expanded
                    final_expanded = current_expanded

                    # 🔄 merge with previous-server expansions
                    logger.debug("  merging with previous servers …")
                    for pre_idx in range(idx - 1, -1, -1):
                        pre_sa = success["server_actions"][pre_idx]
                        logger.info(f"    ↪ previous server: {pre_sa.name}")

                        # (a) previous-server decode expansion
                        pre_decode = pre_sa.server.get_expanded_normalization_with_decode(
                            BASE_NORMALIZATION
                        )
                        # (b) previous-server replace expansion
                        pre_replace = []
                        for t in pre_sa.transformation_list:
                            if isinstance(t.transformation_type, ReplaceTransformation):
                                pre_replace.extend(
                                    pre_sa.server.get_expanded_normalization_with_replace(
                                        t.transformation_type
                                    )
                                )

                        pre_candidates = list(set(pre_decode) | set(pre_replace))
                        logger.debug(
                            f"      candidates from {pre_sa.name}: {len(pre_candidates)}"
                        )

                        # real merge / deduplication
                        final_expanded = get_expanded_normalization_with_pre_server(
                            final_expanded, pre_candidates, pre_sa.server
                        )
                        logger.info(
                            f"      after merging duplicates removed → {len(final_expanded)} candidates"
                        )

                    logger.info(
                        f"  finished merging for {server_action.name} – "
                        f"{len(final_expanded)} final candidates"
                    )

            # store Step-2 result for this successful combination
            expanded_urls = []
            if final_expanded:
                for candidate in final_expanded:
                    expanded_url = original_input_url.replace(
                        BASE_NORMALIZATION.transformation_type.normalization_str,
                        candidate.transformation_type.normalization_str,
                    )
                    expanded_urls.append(expanded_url)

                all_normalization_results.append(
                    {
                        "original_input": original_input_url,
                        "expanded_urls_step2": expanded_urls,
                    }
                )
            else:
                logger.warning("  no final candidates – keeping original URL")
                all_normalization_results.append(
                    {
                        "original_input": original_input_url,
                        "expanded_urls_step2": [original_input_url],
                    }
                )

        else:
            logger.info("Normalization not required – keeping original URL")
            all_normalization_results.append(
                {
                    "original_input": original_input_url,
                    "expanded_urls_step2": [original_input_url],
                }
            )

    # ---------------------------------------------------------------------------
    # Step-2 : summary & timing
    # ---------------------------------------------------------------------------
    logger.info("Step 2 – final normalization results")
    for norm_result in all_normalization_results:
        original_input_url = norm_result["original_input"]
        expansions = norm_result["expanded_urls_step2"]
        logger.info(f"Original URL : {original_input_url}")
        if expansions:
            logger.info(f"  total {len(expansions)} expanded URLs")
            for i, expanded_url in enumerate(expansions, 1):
                logger.debug(f"   {i}. {expanded_url}")
        else:
            logger.warning("  no expanded URLs generated")

    step2_end = time.time()
    log_data["steps"]["step2"]["end"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_data["steps"]["step2"]["duration"] = step2_end - step2_start
    logger.info(
        f"Step 2 finished at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} – elapsed {step2_end - step2_start:.2f}s"
    )

    # Step 3: Condition-based Transformation Expansion
    logger.info("Step 3: Starting condition-based transformation expansion")
    step3_start = time.time()
    log_data["steps"]["step3"] = {"start": datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

    final_transformed_urls = []

    for success in filtered_combinations:
        server_actions = success["server_actions"]
        combo_summary = " | ".join(sa.name for sa in server_actions)

        # generate transformation candidates
        transformation_data = process_transformations(server_actions)

        if not transformation_data:
            logger.info(f"No transformation candidates for combination: {combo_summary}")
        else:
            logger.info(f"Transformation data generated for combination: {combo_summary}")
            logger.debug(f"Transformation data details: {transformation_data}")

        # apply each transformation to each normalized URL
        for norm_result in all_normalization_results:
            for url in norm_result["expanded_urls_step2"]:
                urls_to_replace = [url]
                for orig_str, candidate_list in transformation_data.items():
                    updated = []
                    for current_url in urls_to_replace:
                        if orig_str in current_url:
                            for candidate in candidate_list:
                                new_url = current_url.replace(orig_str, candidate)
                                logger.debug(
                                    f"Replacing '{orig_str}' with '{candidate}' in '{current_url}' -> '{new_url}'"
                                )
                                updated.append(new_url)
                        else:
                            updated.append(current_url)
                    urls_to_replace = updated
                final_transformed_urls.extend(urls_to_replace)

    # summarize final results
    unique_urls = list(set(dict.fromkeys(final_transformed_urls)))  # preserve order, remove duplicates
    if unique_urls:
        logger.info(f"Step 3 completed: {len(unique_urls)} unique transformed URLs generated")
        for idx, url in enumerate(unique_urls, 1):
            logger.debug(f"  {idx}. {url}")
    else:
        logger.warning("Step 3 completed: no transformed URLs generated")

    step3_end = time.time()
    log_data["steps"]["step3"]["end"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_data["steps"]["step3"]["duration"] = step3_end - step3_start
    log_data["final_transformed_urls"] = unique_urls

    os.makedirs(os.path.dirname(output), exist_ok=True)
    # write out full log
    with open(output, "w", encoding="utf-8") as f:
        json.dump(log_data, f, indent=2)
    logger.info(f"Exploit generation result is saved to {output}")


__all__ = ["cli_generate_exploit_payloads"]